{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "13e8b592-0f2e-41e3-8072-1263b445dbe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 ['https://careers.db.com/professionals/search-roles/#/professional/job/55169', 'https://careers.db.com/professionals/search-roles/#/professional/job/54754']\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 111\u001b[0m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;66;03m#cookie try: might not need - site still accessible so should be OK as long as modal not in way of button\u001b[39;00m\n\u001b[0;32m    101\u001b[0m \u001b[38;5;66;03m# try:\u001b[39;00m\n\u001b[0;32m    102\u001b[0m \u001b[38;5;66;03m#     cookie_deny = WebDriverWait(driver, 5).until(\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    108\u001b[0m \n\u001b[0;32m    109\u001b[0m \u001b[38;5;66;03m# Main scraping loop\u001b[39;00m\n\u001b[0;32m    110\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 111\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    112\u001b[0m     outer_loop_break \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    113\u001b[0m     page_source \u001b[38;5;241m=\u001b[39m driver\u001b[38;5;241m.\u001b[39mpage_source\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "# Base URL for company job listings\n",
    "# base_url = 'https://careers.db.com/professionals/search-roles/#/professional/results/'\n",
    "base_url = 'https://careers.db.com/professionals/search-roles/#/professional/results/?title=1'\n",
    "\n",
    "# TODO: Define the pull_company_name function\n",
    "def pull_company_name(base_url, all_links, job_links):\n",
    "    \"\"\"\n",
    "    Extracts job links from the company careers page.\n",
    "\n",
    "    Parameters:\n",
    "    base_url (str): The base URL of the company careers page.\n",
    "    all_links (list): List of all links on the current page.\n",
    "    job_links (list): List of collected job links.\n",
    "\n",
    "    Returns:\n",
    "    list: Updated list of job links.\n",
    "    \"\"\"\n",
    "    # Filter links before appending\n",
    "    # went with list of acceptable locations gathered from currently available cities - bad because we are not accounting for possible new locations \n",
    "    #   elements' only location indicitor on page within anchor tag containing href,\n",
    "    #   no other indicator at moment - may need to switch to automated navigating of filters but sort of difficult at the moment with 2k+ results.\n",
    "    #   site filter cannot handle multiple cities at moment, restricts pattern to: filter -> scrape -> paginate -> scrape -> ... -> filter -> scrape -> paginate ...\n",
    "    acceptable_locations = [\n",
    "        'Sydney', \n",
    "        'Boston', 'Cary', 'Chicago', 'Jacksonville', 'Los Angeles', 'Miami', 'New York', 'San Francisco', 'Santa Ana',\n",
    "        'Birmingham', 'London',\n",
    "        'Beijing', 'Shanghai',\n",
    "        'Hong Kong',\n",
    "        'Singapore'\n",
    "        ]\n",
    "\n",
    "    url_start = base_url[:base_url.find('#')]\n",
    "    # print(url_start)\n",
    "\n",
    "    for link in all_links:\n",
    "        # TODO: Check if the link contains a certain pattern and is not already in job_links\n",
    "        # Append the link to job_links if it meets the criteria\n",
    "        href = link.get('href') # extract href\n",
    "        if href and 'job' in href:\n",
    "            job_url = url_start + href\n",
    "            job_title = link.get_text()\n",
    "            if any(location in job_title for location in acceptable_locations):\n",
    "                if job_url not in job_links:\n",
    "                    job_links.append(job_url)\n",
    "    \n",
    "    # print(len(job_links), job_links)    \n",
    "    return job_links\n",
    "\n",
    "# TODO: Define the button_company_name function\n",
    "def button_company_name(driver, page, outer_loop_break, all_links):\n",
    "    \"\"\"\n",
    "    Handles pagination by clicking the \"next page\"/\"load more\" button.\n",
    "\n",
    "    Parameters:\n",
    "    driver (WebDriver): The Selenium WebDriver instance.\n",
    "    page (int): The current page number. (optional)\n",
    "    outer_loop_break (bool): Flag to indicate when to stop scraping.\n",
    "    all_links (list): List of all links on the current page.\n",
    "\n",
    "    Returns:\n",
    "    tuple: Updated page number and outer_loop_break flag.\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        # TODO: Locate the next page button and navigate to the next page if it exists\n",
    "        load_more = WebDriverWait(driver, 5).until(\n",
    "            EC.element_to_be_clickable((\n",
    "                By.XPATH, \"//button[@class='button type-primary' and text()=' Load more ']\"\n",
    "            ))\n",
    "        )\n",
    "        if load_more:\n",
    "            load_more.click()\n",
    "        else:\n",
    "            outer_loop_break = True\n",
    "            return page, outer_loop_break\n",
    "\n",
    "        # print(load_more[0].text)\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred while trying to click the 'next page' button: {e}\")\n",
    "        outer_loop_break = True\n",
    "    return page + 1, outer_loop_break\n",
    "\n",
    "# WebDriver setup\n",
    "driver = webdriver.Chrome()\n",
    "driver.implicitly_wait(1)\n",
    "driver.get(base_url)\n",
    "\n",
    "# Initialize variables\n",
    "job_links = []\n",
    "prev_page = 0\n",
    "page = 1\n",
    "\n",
    "#cookie try: might not need - site still accessible so should be OK as long as modal not in way of button\n",
    "# try:\n",
    "#     cookie_deny = WebDriverWait(driver, 5).until(\n",
    "#         EC.element_to_be_clickable((By.XPATH, \"//button[@role='button' and @data-testid='uc-deny-all-button' and text()='Deny and continue']\")))\n",
    "#     print(cookie_deny)\n",
    "#     cookie_deny.click()\n",
    "# except Exception as e:\n",
    "#     pass\n",
    "\n",
    "# Main scraping loop\n",
    "while True:\n",
    "    time.sleep(5)\n",
    "    outer_loop_break = False\n",
    "    page_source = driver.page_source\n",
    "    soup = BeautifulSoup(page_source, 'html.parser')\n",
    "    all_links = soup.find_all(\"a\", href=True)\n",
    "\n",
    "    # Scrape job links\n",
    "    job_links = pull_company_name(base_url, all_links, job_links)\n",
    "    # Handle pagination\n",
    "    page, outer_loop_break = button_company_name(driver, page, outer_loop_break, all_links)\n",
    "    # Following if block to be repeated several times\n",
    "    # if block to be filled with a distinctive string that appears in base_url\n",
    "    # if '' in base_url:\n",
    "        # Apply filter here if necessary\n",
    "        \n",
    "    if outer_loop_break:\n",
    "        break\n",
    "\n",
    "# Output the collected job links\n",
    "print(\"Length of job links:\", len(job_links))\n",
    "\n",
    "# Close the WebDriver\n",
    "driver.quit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
